{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/sales.csv')\n",
    "df.dropna(subset=['price'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cost</th>\n",
       "      <th>price</th>\n",
       "      <th>weight</th>\n",
       "      <th>purchase_date</th>\n",
       "      <th>product_type</th>\n",
       "      <th>product_level</th>\n",
       "      <th>maker</th>\n",
       "      <th>ingredient</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$333k</td>\n",
       "      <td>$300,492</td>\n",
       "      <td>3 Ton 90 Kg</td>\n",
       "      <td>Dec 19 2008</td>\n",
       "      <td>Q,B</td>\n",
       "      <td>advanced</td>\n",
       "      <td>M14122</td>\n",
       "      <td>IN732052,IN732053</td>\n",
       "      <td>2.76 meters</td>\n",
       "      <td>97 cm</td>\n",
       "      <td>26 cm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>$430,570</td>\n",
       "      <td>3 Ton 30 Kg</td>\n",
       "      <td>Sep 10 1997</td>\n",
       "      <td>J,D</td>\n",
       "      <td>basic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IN732054,IN732055,IN732056,IN732057,IN732058</td>\n",
       "      <td>2.67 meters</td>\n",
       "      <td>98 cm</td>\n",
       "      <td>26 cm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$270k</td>\n",
       "      <td>$213,070</td>\n",
       "      <td>3 Ton 40 Kg</td>\n",
       "      <td>Sep 05 2001</td>\n",
       "      <td>J,D</td>\n",
       "      <td>basic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IN732054,IN732059,IN732060</td>\n",
       "      <td>3.0 meters</td>\n",
       "      <td>93 cm</td>\n",
       "      <td>24 cm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>$229,174</td>\n",
       "      <td>3 Ton 50 Kg</td>\n",
       "      <td>Dec 23 2016</td>\n",
       "      <td>U</td>\n",
       "      <td>advanced</td>\n",
       "      <td>M14123</td>\n",
       "      <td>IN732061,IN732062,IN732063</td>\n",
       "      <td>2.5 meters</td>\n",
       "      <td>102 cm</td>\n",
       "      <td>27 cm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$97k</td>\n",
       "      <td>$122,659</td>\n",
       "      <td>2 Ton 970 Kg</td>\n",
       "      <td>Jan 12 2000</td>\n",
       "      <td>D,R</td>\n",
       "      <td>advanced</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IN732064,IN732065,IN732066</td>\n",
       "      <td>2.47 meters</td>\n",
       "      <td>101 cm</td>\n",
       "      <td>26 cm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cost     price        weight purchase_date product_type product_level  \\\n",
       "0  $333k  $300,492   3 Ton 90 Kg   Dec 19 2008          Q,B      advanced   \n",
       "1    NaN  $430,570   3 Ton 30 Kg   Sep 10 1997          J,D         basic   \n",
       "2  $270k  $213,070   3 Ton 40 Kg   Sep 05 2001          J,D         basic   \n",
       "3    NaN  $229,174   3 Ton 50 Kg   Dec 23 2016            U      advanced   \n",
       "4   $97k  $122,659  2 Ton 970 Kg   Jan 12 2000          D,R      advanced   \n",
       "\n",
       "    maker                                    ingredient       height   width  \\\n",
       "0  M14122                             IN732052,IN732053  2.76 meters   97 cm   \n",
       "1     NaN  IN732054,IN732055,IN732056,IN732057,IN732058  2.67 meters   98 cm   \n",
       "2     NaN                    IN732054,IN732059,IN732060   3.0 meters   93 cm   \n",
       "3  M14123                    IN732061,IN732062,IN732063   2.5 meters  102 cm   \n",
       "4     NaN                    IN732064,IN732065,IN732066  2.47 meters  101 cm   \n",
       "\n",
       "   depth  \n",
       "0  26 cm  \n",
       "1  26 cm  \n",
       "2  24 cm  \n",
       "3  27 cm  \n",
       "4  26 cm  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = pd.to_datetime(df.purchase_date).dt.year\n",
    "train_raw = df[df.year < 2015]\n",
    "test_raw = df[df.year >= 2015]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Assignment:</font> Get **luxury** (**price** higher than 500k dollars) as targets for training and testing and assign them to variables **y_train** and **y_test**, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = train_raw['price'].map(lambda x: float(x.strip('$').replace(',','') if type(x)==str else x))\n",
    "# y_test = test_raw['price'].map(lambda x: float(x.strip('$').replace(',','') if type(x)==str else x))\n",
    "\n",
    "# y_train = y_train.map(lambda x: 1 if x > 500000 else 0)\n",
    "# y_test = y_test.map(lambda x: 1 if x > 500000 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test data\n",
    "\n",
    "features = list(df.columns)\n",
    "target = [\"price\", \"luxury\"]\n",
    "features = [fea for fea in features if fea not in target]\n",
    "\n",
    "X_train = train_raw[features]\n",
    "X_test = test_raw[features]\n",
    "\n",
    "y_train = train_raw[\"price\"].map(lambda x: 1 if float(x.strip(\"$\").replace(\",\", \"\")) > 500000 else 0)\n",
    "y_test = test_raw[\"price\"].map(lambda x: 1 if float(x.strip(\"$\").replace(\",\", \"\")) > 500000 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Assignment:</font> Build a transformation class to extract, preprocess, and transform **cost**; wrap up this class with **MinMaxScaler** and **LogisticRegression** to build a classification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost_Transformer(object):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        df = pd.DataFrame()\n",
    "        df['cost'] = X.cost.map(self.cost2num)\n",
    "        self.mean = df.mean()\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        df = pd.DataFrame()\n",
    "        df['cost'] = X.cost.map(self.cost2num)\n",
    "        return df.fillna(self.mean)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def cost2num(self, x):\n",
    "        if type(x) == str: \n",
    "            x = x.strip('$').strip('k')\n",
    "            return float(x)*1000\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('ctf', Cost_Transformer()),\n",
    "         ('rescale', MinMaxScaler()),\n",
    "         ('logr', LogisticRegression())]\n",
    "model = Pipeline(steps)\n",
    "model = model.fit(train_raw, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>Note:</font> is rescale necessary here?\n",
    "\n",
    "Not necessary if model performance is considered. But yes if feature importance will be compared. Also, it is necessary to rescale when regularization is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> What is logistic regression? Is logistic regression a regression algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression is used when the dependent variable(target) is categorical. \n",
    "### It measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic/sigmoid function. \n",
    "### Although logistic regression is widely used as a supervised classification algorithm, it by nature still is a regression algorithm. Logistic regression becomes a classification technique only when a decision threshold is brought into the picture. The setting of the threshold value is a very important aspect of logistic regression and is dependent on the classification problem itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> Is logistic regression a linear algorithm? What is the relationship between logistic regression and linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic regression is a linear method because the model is a linear combination of the inputs. Similar to the linear regression, logistic regression predicts a continuous outcome (i.e., the probability of an event). Different from the linear regression, the predictions in logistic regression are transformed using the logistic/sigmoid function such that we can use logistic regression to predict discrete values or categories like True/False.\n",
    "\n",
    "### (1) Linear: $y = a_{0} + a_{1}x + ...$\n",
    "### (2) Logistic: $p = \\frac{e^{b_{0} + b_{1}x}}{1 + e^{b_{0} + b_{1}x}}$  => $log(\\frac{p}{1-p}) = b_{0} + b_{1}x + ...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> What is sigmoid function? What is it for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sigmoid function, also called the logistic function, is a mathematical function that can take any real value and map it to between 0 to 1, but never exactly at those limits. The function takes on a form of: $\\frac{1}{1 + e^{-x}}$. \n",
    "\n",
    "### The sigmoid function is used mostly used in classification type problem since we need to scale the data in some given specific range with a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Prediction with default settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Assignment:</font> Use the **predict** function of the model to make predictions for the training set and test set, and assign the outputs to **y_train_pred** and **y_test_pred**, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(train_raw)\n",
    "y_test_pred = model.predict(test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> What are the shapes of **y_train_pred** and **y_test_pred**? What do the values in **y_train_pred** and **y_test_pred** mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2757,)\n",
      "(429,)\n",
      "(2757,)\n",
      "(429,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_pred.shape)\n",
    "print(y_test_pred.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 means we predict that this is a luxury product (price > 500K), 0 means otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Assignment:</font> Calculate the training and testing accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for the training set: 0.940\n",
      "Accuracy score for the testing set: 0.907\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy score for the training set: {0:.3f}\".\n",
    "      format(accuracy_score(y_train,y_train_pred)))\n",
    "print(\"Accuracy score for the testing set: {0:.3f}\".\n",
    "      format(accuracy_score(y_test,y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Assignment:</font> Calculate the precision, recall, and f1 scores. You can use classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97      2573\n",
      "           1       0.70      0.17      0.27       184\n",
      "\n",
      "    accuracy                           0.94      2757\n",
      "   macro avg       0.82      0.58      0.62      2757\n",
      "weighted avg       0.93      0.94      0.92      2757\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.99      0.95       383\n",
      "           1       0.71      0.22      0.33        46\n",
      "\n",
      "    accuracy                           0.91       429\n",
      "   macro avg       0.81      0.60      0.64       429\n",
      "weighted avg       0.89      0.91      0.88       429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Assignment:</font> Get the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TN FP <br>\n",
    "FN TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2560   13]\n",
      " [ 153   31]]\n",
      "[[379   4]\n",
      " [ 36  10]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    415\n",
       "1     14\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.DataFrame(y_test_pred)\n",
    "predictions[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> Is this model better than the all-positive and all-negative models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Based on wegithed average f1 score, this model (0.92 train, 0.89 test) performs better than the all-negative model (0.90 train, 0.84 test) and the all-positive model (0.01 train, 0.02 test).\n",
    "\n",
    "We also observed that this model performs better when predicting negative samples (label 0) than predicting positive samples (label 1) based on all the metrics (precision, recall, and f1-score). I.e., prediction is biased to the majority class due to data imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Prediction with balanced class weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Assignment:</font> Set class_weight in Logistic Regression and retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('ctf', Cost_Transformer()),\n",
    "         ('rescale', MinMaxScaler()),\n",
    "         ('logr', LogisticRegression(class_weight = 'balanced'))]\n",
    "model = Pipeline(steps)\n",
    "model = model.fit(train_raw, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(train_raw)\n",
    "y_test_pred = model.predict(test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Assignment:</font> Re-calcualte all the above metrics and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for the training set: 0.858\n",
      "Accuracy score for the testing set: 0.872\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score for the training set: {0:.3f}\".\n",
    "      format(accuracy_score(y_train,y_train_pred)))\n",
    "print(\"Accuracy score for the testing set: {0:.3f}\".\n",
    "      format(accuracy_score(y_test,y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.87      0.92      2573\n",
      "           1       0.28      0.71      0.40       184\n",
      "\n",
      "    accuracy                           0.86      2757\n",
      "   macro avg       0.63      0.79      0.66      2757\n",
      "weighted avg       0.93      0.86      0.88      2757\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.92       383\n",
      "           1       0.44      0.76      0.56        46\n",
      "\n",
      "    accuracy                           0.87       429\n",
      "   macro avg       0.71      0.82      0.74       429\n",
      "weighted avg       0.91      0.87      0.89       429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_train_pred))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2236  337]\n",
      " [  54  130]]\n",
      "[[339  44]\n",
      " [ 11  35]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    350\n",
       "1     79\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.DataFrame(y_test_pred)\n",
    "predictions[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> Do you see any difference? Why is the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After we balance the data class distribution, the accuracy and precision (macro avg) decreased, which is to be expected. The recall increased because of the decreasing in FN (macro avg). The new confusion matrix also shows an increase in TP (and FP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For how class_weight works: It penalizes mistakes in samples of class[i] with class_weight[i] instead of 1. So higher class-weight means you want to put more emphasis on a class. It basically means replicating the smaller class until you have as many samples as in the larger one, but in an implicit way. (Ref: https://stackoverflow.com/questions/30972029/how-does-the-class-weight-parameter-in-scikit-learn-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> Which results are more reasonable? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The balanced result is more reasonable. Standard classifier algorithms like Decision Tree and Logistic Regression have a bias against the classes that have small number of instances. They tend to only predict the majority class data. The features of the minority class are treated as noise and are often ignored. Thus, there is a high probability of misclassification of the minority class as compared to the majority class. Therefore, we need to balance the data class distribution first before building any model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> What are the other methods to deal with imbalanced data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Random Up-sampling and down-sampling the training data\n",
    "### (2) Cluster-based Up-sampling\n",
    "### (3) Modifying existing classification algorithms to make them appropriate for imbalanced data sets. The approach involves constructing several two stage classifiers from the original data and then aggregate their predictions. (Bagging based and Boosting based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions \n",
    "\n",
    "https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-logistic-regression/\n",
    "\n",
    "First, binary logistic regression requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal.\n",
    "\n",
    "Second, logistic regression requires the observations to be independent of each other.  In other words, the observations should not come from repeated measurements or matched data.\n",
    "\n",
    "Third, logistic regression requires there to be little or no multicollinearity among the independent variables.  This means that the independent variables should not be too highly correlated with each other.\n",
    "\n",
    "Fourth, logistic regression assumes linearity of independent variables and log odds.  although this analysis does not require the dependent and independent variables to be related linearly, it requires that the independent variables are linearly related to the log odds.\n",
    "\n",
    "Finally, logistic regression typically requires a large sample size.  A general guideline is that you need at minimum of 10 cases with the least frequent outcome for each independent variable in your model. For example, if you have 5 independent variables and the expected probability of your least frequent outcome is .10, then you would need a minimum sample size of 500 (10*5 / .10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "log loss, -(y*log(p) + (1-y) *log(1-p))\n",
    "\n",
    "Why cannot the cost function used for linear regression be used for logistic? Linear regression uses mean squared error as its cost function. If this is used for logistic regression, then it will be a non-convex function of parameters. Gradient descent will converge into global minimum only if the function is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with imbalanced data (extreme base rate)\n",
    "- Data: resample the data, balance the majority and minority data size.\n",
    "- Loss function: class_weight = balanced, penalize more for not correctly identifying minority class.\n",
    "- Model: ensemble, bagging or boosting.\n",
    "- Metrics: roc_auc score, precision, and recall score.\n"
   ]
  },
  {
   "attachments": {
    "5db9e29a-d6c4-4c84-9818-e9bb30d679e3.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you want to read more ...\n",
    "\n",
    "Ref: https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\n",
    "\n",
    "Most machine learning algorithms work best when the number of samples in each class are about equal. This is because most algorithms are designed to maximize accuracy and reduce error.\n",
    "\n",
    "5 Techniques to Handle Imbalanced Data:\n",
    "1. Change the performance metric\n",
    "    - Confusion Matrix: a table showing correct predictions and types of incorrect predictions.\n",
    "    - Precision: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier’s exactness. Low precision indicates a high number of false positives.\n",
    "    - Recall: the number of true positives divided by the number of positive values in the test data. Recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier’s completeness. Low recall indicates a high number of false negatives.\n",
    "    - F1: Score: the weighted average of precision and recall.\n",
    "2. Change the algorithm\n",
    "    - While in every machine learning problem, it’s a good rule of thumb to try a variety of algorithms, it can be especially beneficial with imbalanced datasets.\n",
    "3. Resampling Techniques — Oversample minority class\n",
    "4. Resampling techniques — Undersample majority class\n",
    "\n",
    "Ref: https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\n",
    "\n",
    "Ref: https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/\n",
    "\n",
    "2.1 Data resampling\n",
    "\n",
    "2.1.3  Cluster-Based Over Sampling\n",
    "\n",
    "In this case, the K-means clustering algorithm is independently applied to minority and majority class instances. This is to identify clusters in the dataset. Subsequently, each cluster is oversampled such that all clusters of the same class have an equal number of instances and all classes have the same size.  \n",
    "\n",
    "Advantages\n",
    "- This clustering technique helps overcome the challenge between class imbalance. Where the number of examples representing positive class differs from the number of examples representing a negative class.\n",
    "- Also, overcome challenges within class imbalance, where a class is composed of different sub clusters. And each sub cluster does not contain the same number of examples.\n",
    "\n",
    "Disadvantages\n",
    "- The main drawback of this algorithm, like most oversampling techniques is the possibility of over-fitting the training data.\n",
    "\n",
    "2.1.4  Informed Over Sampling: Synthetic Minority Over-sampling Technique for imbalanced data\n",
    "\n",
    "This technique is followed to avoid overfitting which occurs when exact replicas of minority instances are added to the main dataset. A subset of data is taken from the minority class as an example and then new synthetic similar instances are created. These synthetic instances are then added to the original dataset. The new dataset is used as a sample to train the classification models.\n",
    "\n",
    "Advantages\n",
    "- Mitigates the problem of overfitting caused by random oversampling as synthetic examples are generated rather than replication of instances\n",
    "- No loss of useful information\n",
    "\n",
    "Disadvantages\n",
    "- While generating synthetic examples SMOTE does not take into consideration neighboring examples from other classes. This can result in increase in overlapping of classes and can introduce additional noise\n",
    "- SMOTE is not very effective for high dimensional data\n",
    "\n",
    "2.1.5  Modified synthetic minority oversampling technique (MSMOTE) for imbalanced data\n",
    "\n",
    "It is a modified version of SMOTE. SMOTE does not consider the underlying distribution of the minority class and latent noises in the dataset. To improve the performance of SMOTE a modified method MSMOTE is used.\n",
    "\n",
    "2.2 Algorithmic Ensemble Techniques\n",
    "\n",
    "The main objective of ensemble methodology is to improve the performance of single classifiers. The approach involves constructing several two stage classifiers from the original data and then aggregate their predictions.\n",
    "\n",
    "![image.png](attachment:5db9e29a-d6c4-4c84-9818-e9bb30d679e3.png)\n",
    "\n",
    "2.2.1. Bagging Based techniques for imbalanced data\n",
    "\n",
    "Bagging is an abbreviation of Bootstrap Aggregating. The conventional bagging algorithm involves generating ‘n’ different bootstrap training samples with replacement. And training the algorithm on each bootstrapped algorithm separately and then aggregating the predictions at the end.\n",
    "\n",
    "Bagging is used for reducing Overfitting in order to create strong learners for generating accurate predictions. Unlike boosting, bagging allows replacement in the bootstrapped sample.\n",
    "\n",
    "Total Observations = 1000, Fraudulent   Observations =20, Non Fraudulent Observations = 980, Event Rate= 2 %. There are 10 bootstrapped samples chosen from the population with replacement. Each sample contains 200 observations. And each sample is different from the original dataset but resembles the dataset in distribution & variability.\n",
    "\n",
    "The machine learning algorithms like logistic regression, neural networks, decision tree  are fitted to each bootstrapped sample of 200 observations. And the Classifiers c1, c2…c10 are aggregated to produce a compound classifier.  This ensemble methodology produces a stronger compound classifier since it combines the results of individual classifiers to come up with an improved one.\n",
    "\n",
    "Advantages\n",
    "- Improves stability & accuracy of machine learning algorithms\n",
    "- Reduces variance\n",
    "- Overcomes overfitting\n",
    "- Improved misclassification rate of the bagged classifier\n",
    "- In noisy data environments bagging outperforms boosting\n",
    "\n",
    "Disadvantages\n",
    "- Bagging works only if the base classifiers are not bad to begin with. Bagging bad classifiers can further degrade performance\n",
    "\n",
    "2.2.2. Boosting-Based techniques for imbalanced data\n",
    "\n",
    "Boosting is an ensemble technique to combine weak learners to create a strong learner that can make accurate predictions. Boosting starts out with a base classifier / weak classifier that is prepared on the training data. The base learners / Classifiers are weak learners i.e. the prediction accuracy is only slightly better than average. A classifier learning algorithm is said to be weak when small changes in data induce big changes in the classification model.\n",
    "\n",
    "In the next iteration, the new classifier focuses on or places more weight to those cases which were incorrectly classified in the last round."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\n",
    "\n",
    "Types of Logistic Regression\n",
    "1. Binary Logistic Regression: The categorical response has only two 2 possible outcomes. Example: Spam or Not\n",
    "2. Multinomial Logistic Regression: Three or more categories without ordering. Example: Predicting which food is preferred more (Veg, Non-Veg, Vegan)\n",
    "3. Ordinal Logistic Regression: Three or more categories with ordering. Example: Movie rating from 1 to 5\n",
    "\n",
    "Decision Boundary\n",
    "- Decision boundary can be linear or non-linear. Polynomial order can be increased to get complex decision boundary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
